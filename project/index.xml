<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | </title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 07 Jul 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>Forward</title>
      <link>/project/forward/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/project/forward/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pool&#43;&#43;</title>
      <link>/project/pool&#43;&#43;/</link>
      <pubDate>Sun, 27 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/project/pool&#43;&#43;/</guid>
      <description>&lt;p&gt;Best In Show and People&amp;rsquo;s Choice at San Francisco Science Hack Day 2015.&lt;/p&gt;
&lt;p&gt;This was a father (hardware) and son (software) project done over 2 days.  We
instrumented a pool table with a LIDAR and an overhead camera — determining the
precise location of the balls and the cue using computer vision and time of
flight data.  These coordinates were fed into a physics engine which would
predict the outcome of the shot, which were in turn drawn onto the table using a
high-powered laser.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building Detection</title>
      <link>/project/satellite/</link>
      <pubDate>Tue, 10 Jun 2014 00:00:00 +0000</pubDate>
      <guid>/project/satellite/</guid>
      <description>&lt;p&gt;In this project we used a densely connected graphical model to detect building
rooftops in satellite imagery. The problem was formulated as a pixel-level image
segmentation task. Labeled building data was pulled from OpenStreetMap and
registered to publicly available satellite imagery using standard GIS
techniques.&lt;/p&gt;
&lt;p&gt;We applied a fully connected conditional random field (CRF)
defined on the complete set of pixels of an image as proposed by 
&lt;a href=&#34;https://arxiv.org/abs/1210.5644&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Krahenbuhl and
Koltun&lt;/a&gt;, with unary potentials derived from
Shotton et al.’s

&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ijcv07a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TextonBoost&lt;/a&gt;.
Inference in this model is made tractable by using mean-field approximation and
Gaussian kernels for the pairwise potentials, allowing for an efficient
message-passing algorithm.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Expression Recognition</title>
      <link>/project/expression/</link>
      <pubDate>Sat, 15 Dec 2012 00:00:00 +0000</pubDate>
      <guid>/project/expression/</guid>
      <description>&lt;p&gt;Classified which of 6 expressions (Joy, Sorrow, Disgust, Anger, Surprise, Fear)
were being displayed in grayscale images of subjects’ faces.&lt;/p&gt;
&lt;p&gt;We trained one-vs-many linear SVMs on Gabor filters applied to the input images.
We used existing standard datasets (i.e., Cohn-Kanade and JAFFE) as well as a
self-collected 47-subject dataset: the Berkeley Students and Friends (BSAF).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FutureHand</title>
      <link>/project/futurehand/</link>
      <pubDate>Thu, 02 Sep 2010 00:00:00 +0000</pubDate>
      <guid>/project/futurehand/</guid>
      <description>&lt;p&gt;The FutureHand was inspired by Pranav Mistry&amp;rsquo;s 2009 
&lt;a href=&#34;https://www.youtube.com/watch?v=YrtANPtnhyg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TED
talk&lt;/a&gt; on the Sixth Sense device.
I was interested in exploring novel human computer interfaces and felt that the
ubiquitous 2-DOF mouse could be augmented to instead operate in 6 dimensions:
the 3 translational (x, y, z) and the 3 rotational dimensions (yaw, pitch,
roll).&lt;/p&gt;
&lt;p&gt;The FutureHand is a home-brew inertial measurement unit (IMU) integrated into a
bluetooth USB device. A pair of gyroscopes were used to capture rapid rotational
changes, as well as measure the absolute rotations through dead reckoning. An
accelerometer and magnetometer were used to measure translational changes, as
well as combined with the gyroscope rotation estimates through complementary
filters to allow for more accurate absolute rotations.&lt;/p&gt;
&lt;p&gt;The FutureHand was then augmented with a Bluetooth chip, and inserted into a
custom molded case, allowing for easy use as a 6-DOF mouse. This novel
controller was integrated into the 
&lt;a href=&#34;/project/moonweasel/&#34;&gt;Moonweasel&lt;/a&gt; project, the 3-dimensional space flight simulator, allowing for more
precise flight control.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;casing.jpg&#34; alt=&#34;futurehand casing&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MoonWeasel</title>
      <link>/project/moonweasel/</link>
      <pubDate>Wed, 02 Jun 2010 00:00:00 +0000</pubDate>
      <guid>/project/moonweasel/</guid>
      <description>&lt;p&gt;An early project in my programming career, I led a team of half a dozen high
school students in the writing of a 3D space flight simulator. Different
subgroups successfully built in multiplayer capabilities, a custom physics
engine, and 3D models.  Unfortunately, documentation was not our forte, and
the above image is one of the few records we have remaining.&lt;/p&gt;
&lt;p&gt;The simulator was later integrated into the 
&lt;a href=&#34;/project/futurehand/&#34;&gt;FutureHand&lt;/a&gt; project.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
